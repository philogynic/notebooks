{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Technology Learning Path\n",
    "\n",
    "This is a story about how i learn big data technology, especially data engineering part. Why I want to learn it? It is because I want to really deploy full machine learning and analytics system into a production. It is called machine learning engineer right now, and you need both software engineering and data science skills and experience.\n",
    "\n",
    "So what are the things you need to learn hey you 'unicorn' wannabe? You not only need statistics and mathematical models dude, you also need to become strong programmers, understand data ingestion, data cleaning, prototyping, bringing prototype into production, product design, setting up and managing infrastructure, and much more.\n",
    "\n",
    "How to get started dude. I mean there's alot of shits right? It just like spells or pokemon names.\n",
    "\n",
    "Hadoop, Distributed computing, Kafka, NoSQL, or Spark\n",
    "\n",
    "There is two categories:\n",
    "- Big data engineering\n",
    "- Big data analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important from quora:\n",
    "Data engineer and engineering for ML are almost completely different things\n",
    "\n",
    "I am the first Machine Learning Engineer hired in our team of Data Scientists. I just started working in this role, so take my comment with a grain of salt. Much of my job helps fill a role that many people in my team would rather not do or not skilled to do. I collaborate closely with the Data Scientists and transforming what they wrote as a Notebook or a Python script into something that can be deployed. This often involves wrapping the training and classification into a Flask microservice, scaling the service, containerizing it, adding monitoring, tests, and other requirements for production. I often also work closely with the data engineering teams with improving ETL tasks, as well. Soon, my role will be around building infrastructure and tooling that can be used for future projects and maybe open sourced. I think that at many companies currently, there are many Data Scientists who are more data focused and some more infrastructure focused and creating new roles helps provide clarification of responsibilities between them.\n",
    "\n",
    "I think that currently in the industry, the responsibilities in the DS roles are somewhat broad and that’s why we are seeing more roles opening up for particular directions based on what people want to do or trained to do. Many of the Data Scientists that I work with have hardly any programming experience at all except for working with R- but they have PhDs or Masters in Statistics. They don’t know how to use version control or collaborate on large scale software projects, but they are amazing modelers and know a tremendous amount of mathematics. Some people I work with have PhDs in Physics also are better programmers than I am. I was told that why I was a competitive candidate for the Machine Learning Engineer position is that I recently came from a Software Engineer position where I was a team lead and ScrumMaster and my skills in Agile, Scrum, writing test cases, CI/CD, DevOps (Docker+Kubernetes), etc- and that can help make our Data Science team more effective in the organization. I did my PhD in Computer Science with sensor networks and ubiquitous computing which had a strong Machine Learning component and that certainly helps with communication and collaboration with the data science team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data engineering\n",
    "Design, deployment, acquiring and maintenance of large amount of data. They making the system (ecosystem) of relevant data that can be used to consumer facing or internal. Data engineers aren't primarily mathematicians or statisticians, though they're comfortable with math and statistics; they aren't primarily software developers, though they're comfortable with software. Data engineers are responsible for the operation and maintenance of the data stack. They can take a prototype that runs on a laptop and make it run reliably in production. They're responsible for understanding how to build and maintain a Hadoop or Spark cluster, together with the many other tools that are part of the ecosystem: databases (such as HBase and Cassandra), streaming data platforms (Kafka, Spark Streaming, Apache Flink), and many more moving parts. They know how to do operations in the cloud, taking advantage of Amazon Web Services, Microsoft Azure, and Google Compute Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analytics\n",
    "I know this. But still never big. Basically utilizing data that are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Engineer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About me:\n",
    "I know, I like analytics more that data engineering. But still I want to get the picture of how those fucking technology is work? What is the benefit of using them?\n",
    "\n",
    "It's just like when I really want to learn how web app works. After resigned from my first job, I commit myself to learn more about building web app. So I did whole practical example from several tutorial on website and books to learn to build one using flask, django, and PHP. It feels really good after I finish that. That mysterious things turns out to be very simple. Plus i learn more about backend and database design and why is it important.\n",
    "\n",
    "The drawbacks tho is about how I learn things. I need to really do it to understand it. And to really try things needs time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So because for me Data Engineering is important, I have to learn them. Just for the sake of getting picture. So How to be a data engineer?\n",
    "\n",
    "### Learning some jargon:\n",
    "### Data requirements\n",
    "- __Structure__ (structured/unstructured)\n",
    "- __Size__ (S/M/L/Streaming)\n",
    "- __Sink Throughput__ rate of data can be accepted\n",
    "- __Source Throughput__ rate of data can be updated\n",
    "\n",
    "### Processing requirements\n",
    "- __Query time__ The time that a system takes to execute queries\n",
    "- __Processing time__ Time required to process data\n",
    "- __Precision__ The accuracy of data processing\n",
    "\n",
    "### System and architecture\n",
    "Scenario: Design a system for analyzing sales performance of a company by __creating a data lake__ from multiple data sources like customer data, leads data, call center data, sales data, product data, weblogs.\n",
    "\n",
    "### Learn to design solutions and technologies\n",
    "Solution: Data lakes for sales\n",
    "\n",
    "Big data system must:\n",
    "- seamlessly __integrate data__ from various sources.\n",
    "- make __utilization__ of data easy and fast\n",
    "\n",
    "Defining end goal:\n",
    "- create data lake by integrating data from multiple sources.\n",
    "- automated updates of the data at regular interval of time\n",
    "- data availability of analysis\n",
    "- architecture for easy access and seamless deployment\n",
    "\n",
    "so the solution can be:\n",
    "- __Data related requirements__: most data is structured and has a defined data model. But data sources like weblogs, customer interactions/ call center data, image data from the sales catalog, product advertising data might be unstructured. So the data might be structured and unstructured. The size are L or XL (choice Hadoop), sink throughput (High), quality medium (Hadoop & Kafka), completeness Incomplete.\n",
    "\n",
    "- __Processing related requirements__: As multiple data sources are being integrated, it is important to note that different data will enter the system at different rates. For example the weblogs will be available in a continuous stream with a high level of granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list to learn:\n",
    "- Bash scripting : But what is the connection between bash scripting and \n",
    "    - The linux command line: A complete introduction, William E. Shotts Jr.\n",
    "- Python or Java\n",
    "    - Learn OOP python, Pandas\n",
    "- Cloud\n",
    "    - JUST TRY AWS\n",
    "- HDFS (Hadoop Distributed File System)\n",
    "    - To get the concept, read this\n",
    "    - To get more technical, read this\n",
    "- Kafka\n",
    "- Mapreduce\n",
    "- SQL\n",
    "- Procedural languange\n",
    "- Hive\n",
    "- Pig\n",
    "- Storm\n",
    "- Kinesis\n",
    "- Spark Streaming\n",
    "- Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2017/03/big-data-learning-path-for-all-engineers-and-data-scientists-out-there/\n",
    "- https://www.quora.com/What-do-I-need-to-learn-to-switch-from-data-science-to-data-engineering\n",
    "- http://blog.udacity.com/2016/04/5-skills-you-need-to-become-a-machine-learning-engineer.html\n",
    "- https://www.quora.com/What-is-difference-between-a-Data-Scientist-and-a-Machine-Learning-Engineer-Which-is-better-in-terms-of-salary-and-long-term-growth-and-why-How-are-they-correlated\n",
    "- https://www.quora.com/What-is-the-difference-between-a-machine-learning-engineer-and-a-data-scientist-at-Quora-Do-they-work-together-often-In-what-capacity\n",
    "- https://www.oreilly.com/ideas/what-are-machine-learning-engineers\n",
    "- https://www.oreilly.com/ideas/what-is-hardcore-data-science-in-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Marck Vaisman | Scaling up to Big Data Devops for Data Science ](https://www.youtube.com/watch?v=pzHwAuTMpeU)\n",
    "\n",
    "My ideal environtment for big data:\n",
    "- Latest version of Hadoop, and associated services (YARN, etc.), PIG, Hive, Spark\n",
    "- R with loads of packages\n",
    "- RStudio\n",
    "- Anaconda Python\n",
    "- sparkR, PySpark, sparklyr, mrjob\n",
    "- Git configured with your GitHub\n",
    "- Codecs (zip, gzip, lzop)\n",
    "- Curl, wget, etc\n",
    "- Access to data on HDFS for last mile analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jon Bodner | You got you engineering in my Data Science: Addressing the reproducibility crisis](https://www.youtube.com/watch?v=U1dg1QwLp98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Andy Terrel | Dev Ops meets Data Science: Taking models from prototype to production with Docker and Kubernetes](https://www.youtube.com/watch?v=i8vrWFZW2xk)\n",
    "\n",
    "- Build API\n",
    "- API Uptime\n",
    "- API Response time\n",
    "- Hosting Options\n",
    "    - PAAS (platform as a service): Heroku\n",
    "    - IAAS (infrastructure as a service)\n",
    "- Service level architecture\n",
    "    - lambda architecture\n",
    "    - microservice architecture\n",
    "    - monolytics\n",
    "- DevOps solution\n",
    "    - Program on your laptop\n",
    "        - Anaconda\n",
    "    - Ship your code to your production system\n",
    "        - Docker: shipping container for code\n",
    "    - Manage your services\n",
    "        - Kubernetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Indeedeng: From Data to Deployment-Full Stack Data Science](https://www.youtube.com/watch?v=LLvvNNWp3D0)\n",
    "- What are the things that data scientist (full-stack) do in indeed?\n",
    "    - Labeling data\n",
    "        - the best way to understand your problem is to label your own data\n",
    "        - the fastest way to get labels for your data is to label your own data\n",
    "        - the easiest way to know your labels are consistent is to label your own data\n",
    "        - labeling encourages feature development\n",
    "        - create human benchmark (human perform on the task)\n",
    "        - labeling throughout gives you indication of shifting data\n",
    "        - **create tools for labeling task**\n",
    "        - train others to label your data, or use expert, check their consistency\n",
    "        - always flag weird data (spam)\n",
    "    - Extracting features in one place\n",
    "        - prevent inconsistencies of features in training\n",
    "        - allows faster feature iteration\n",
    "        - Encourages feature extraction reuse\n",
    "        - deploy feature extraction services\n",
    "    - Reuse your model building code\n",
    "        - Output in standard format\n",
    "        - Deploy model\n",
    "    - Release softly and log everything\n",
    "        - can reuse logs for future models\n",
    "        - can gives insight changing over data\n",
    "        - allows to see what went wrong\n",
    "    - Validate and review every model\n",
    "        - Model context: what should this model enable us to do (highlighting, filtering, sorting)? What products / interfaces / workflows will initially use this model?\n",
    "        - Data: What queries and filters were used? From what time range did your data originate? Did you sample your dataset?\n",
    "        - Response variable: what we're trying to predict? How was the response variable labeled or collected? What the model outputs (predictions) represent and how they should be scaled and thresholded?\n",
    "        - Features: Why that features? How were your features generated? which features were most important?\n",
    "        - Model selection and performance: Performance reports on train / test sets. Overall CV search strategy and scoring function. Expected model performance.\n",
    "        - Transparency and recommendations: good documentation\n",
    "            - properties files for model builder\n",
    "            - link to branch of model builder\n",
    "            - examples of model predictions\n",
    "            - possible direction for future improvement\n",
    "            - a couple sentences on why you think the model is ready for production\n",
    "        - Monitor after deployment\n",
    "            - features and data are hard dependencies\n",
    "            - need a post deploy plan (what happens if...)\n",
    "            - use log data to check for features changes\n",
    "            - make tools for this\n",
    "        - Retraining a model\n",
    "            - features monitoring tools\n",
    "            - avoid counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docker for Data Scientists, Strata 2016, Michelangelo D'Agostino](https://www.youtube.com/watch?v=GOW6yQpxOIg)\n",
    "\n",
    "Building pug classifier\n",
    "\n",
    "Want to learn more about docker?\n",
    "- [Building Python apps with Docker](https://www.youtube.com/watch?v=VhabrYF1nms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Andrew T Baker | 5 ways to deploy your Python web app in 2017 Pycon 2017](https://www.youtube.com/watch?v=vGphzPLemZE)\n",
    "\n",
    "1. ngrok\n",
    "    - pros: fast and easy, handy for demos, hacking on webhooks\n",
    "    - cons: stops when you close your laptop, random domains, definitely doesn't scale\n",
    "- heroku (paas)\n",
    "    - pros: one app 24/7 for free, zero server management, add-ons ecosystem\n",
    "    - cons: scaling is easy but gets pricy, harder server cust, some add-ons better than others\n",
    "- \"serverless\": only run when someone accessing: AWS lambda + Zappa (sort of wrapper)\n",
    "    - pros: economical for small to medium loads, good for spikey traffic, zero server config\n",
    "    - cons: relatively new technique, less fun without Zappa, can be tricky to troubleshoot\n",
    "- virtual machines: Google cloud platform\n",
    "    - pros: full control, scales as much as your wallet, economical\n",
    "    - cons: more work for you, there's a lot more to learn, harder to predict ultimate costs\n",
    "- docker\n",
    "    - pros: helps with dev / prod parity, nice for microservices, impress your friend\n",
    "    - cons: newest technique, has it's own learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aku ngantuk\n",
    "\n",
    "https://devcenter.heroku.com/articles/getting-started-with-python#introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
