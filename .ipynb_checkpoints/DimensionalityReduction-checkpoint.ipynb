{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "- PCA (Principal Component Analysis), unsupervised\n",
    "- LDA (Linear Discriminant Analysis), supervised\n",
    "- Kernel PCA, for non-linear\n",
    "- t-SNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Put simple, in this framework, we gonna transform or project the data onto new feature space. And there is important point :underlying difference with feature selection algo that is in this context, we maintain most relevant information. But isn't it just the same? I think in feature selection we can loss the whole feature. \n",
    "\n",
    "Popular application of PCA:\n",
    "- exploratory data analysis\n",
    "- de-noising signal in stock market trading.\n",
    "- Can help indentify pattern in data based on the correlation between features.\n",
    "\n",
    "In nutshell, PCA find the direction of maximum variance and project the data onto that new axes (equal or fewer dimension than the original). The orthogonal axes (called principal component) can be interpreted as the directions of maximum variance. That means, the resulting model would be just a matrix transformation that able to maps the original point with feature vector $\\pmb{x}$ onto its new lower dimension space. The first component would have the largest variance, the second would be the second largest.\n",
    "\n",
    "Important: PCA need standardization. YEah that obvious. Why obvious? The scale would affect the variance calculation. We need sort of equal importance of them\n",
    "\n",
    "PCA steps:\n",
    "1. standardize the $d$-dimensional dataset.\n",
    "- construct the covariance matrix $d \\times d$-dimensional between features.\n",
    "- decompose the covariance matrix into eigenvector and eigenvalues. WTF?\n",
    "- select $k$ eigenvector that correspond to the $k$ largest eigenvalues, where $k$ is the dimensionality of the new features. ic2\n",
    "- construct projection matrix $W$ from the top $k$ eigenvectors\n",
    "- transform the input feature vector using the projection matrix into their new subspace.\n",
    "\n",
    "What is covariance? ITs variance but with Co infront of it. what is intuition tho. It just like the gradient right? that tells the relationship between two variable. The formula looks like this\n",
    "\n",
    "$$\\sigma_{jk} = \\frac{1}{n}\\sum_{i=1}^n \\left( x_j^{(i)} -\\mu_j \\right)\\left( x_k^{(i)} - \\mu_k \\right)$$\n",
    "\n",
    "How the formula can be like that? Positive covariance indicates that the two features increase or decrease together. The covariance matrix would be $d\\times d$ and represented by simbol $\\Sigma$. Now, how about that eigenvalue and eigenvector things?\n",
    "\n",
    "$$\\Sigma \\pmb{v} = \\lambda \\pmb{v}$$\n",
    "\n",
    "with $\\pmb{v}$ is eigenvector and $\\lambda$ is the eigenvalue. In Numpy, we can use `linalg.eig` to get both vector and value. The API would look like this: `eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)`\n",
    "\n",
    "After that, take a view ($k$) of eigenvector with highest value of eigenvalues. That would be $k$ most informative eigenvector. It explain the variance. ITs magic, how from covariance matrix and using eigen formula would result in informative vector? Eigenvector would have length $d$. If we want to reduce only to two dimension, we take 2 most informative eigenvector. That would be our transformation matrix $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
